\subsubsection{\bf RQ1. Comparison with State-of-the-Art DL-based FL Approaches.}
\label{sec:rq1-result}

\begin{table}[t]
	\caption{RQ1. Comparison Results with DL-based FL Models}
        \vspace{-6pt}
	{\small
		\begin{center}
			\renewcommand{\arraystretch}{1}
				\begin{tabular}{p{1.5cm}<{\centering}|p{1cm}<{\centering}|p{0.8cm}<{\centering}|p{1.2cm}<{\centering}|p{1.2cm}<{\centering}}
				\hline
				Metrics & CNN-FL & DeepFL & DeepRL4FL & \tool \\			
				\hline
				Hit-1@Set   & 332 & 331 & 352 & 387 \\
				Hit-2@Set	& 125 & 119 & 132 & 167 \\
				Hit-3@Set	& 57 & 56 & 64 & 82 \\
				Hit-4@Set	& 23 & 18 & 28 & 46 \\
				Hit-5@Set	& 5 & 4 & 6 & 9 \\
				Hit-5+@Set	& 1 & 2 & 3 & 3 \\
				EXAM     	& 0.09 & 0.10 & 0.09 & 0.07 \\
				\hline
			\end{tabular}
			
			\label{fig:rq1-0}
		\end{center}
	}
\end{table}

As seen in Table~\ref{fig:rq1-0}, the comparison results show that
{\tool} can improve over all baselines on locating CC fixing
statements in every evaluation metric. Particularly, {\tool} can
improve CNN-FL, DeepFL, and DeepRL4FL by 16.6\%, 16.9\%, and 9.9\%,
respectively, in terms of Hit-1@Set (the predicted set contains at
least one faulty statement).

Hit-N@Set with N$\geq$2 aims to evaluate the capability of detecting
multiple CC fixing statements. As seen, {\tool} locates more multiple
statements in a set than any baseline. Specifically, {\tool} can
improve CNN-FL, DeepFL, and DeepRL4FL by 33.6\%, 40.3\%, and 26.5\% in
terms of Hit-2@Set, 43.9\%, 46.4\%, and 28.1\% in terms of Hit-3@Set,
100\%, 155.6\%, and 64.5\% in terms of Hit-4@Set, respectively.
%
The sizes of our predicted sets range from 1 to 8 statements with a
median size of 3 and a mean size of 3.48. 75\% of the sets have a size
from 3-5 statements.
%6.2\% for 6-8 and 18.9\% for 1-2, 74.9\% for 3-5 
Thus, {\tool} detects CC fixing locations well for the common cases in
terms of the number of faulty statements, while it is still
challenging for all models for the cases of more than 5 faulty
statements.

%Thus, Hit-5@Set and Hit5+@Set are low as they are already larger than
%our predicted sets. However, all other baselines also cannot work on
%the cases with N$\geq$5.

{\tool} also reduces EXAM scores compared to the baselines CNN-FL,
DeepFL, and DeepRL4FL by 22.2\%, 30\%, and 22.2\%, respectively. Thus,
{\tool} saves more human efforts in searching for the CC fixing
statements than the baselines.



Table~\ref{fig:rq1-1} shows the breakdown results on two metrics
Hit-N@Set and EXAM Scores for different types of faults with {\em
different numbers of CC fixing statements (i.e., faulty
statements)}. We classify the bugs into 6 types based on the number of
CC fixing statements: bugs with $K$ faulty statements, $K$ = 1, 2, 3,
4, 5, and 5+. As seen, {\tool} improves over the baselines in any
metric when dealing with any number of faulty statements. For the bugs with $\leq$5 faulty statements, {\tool} locates more
faulty statements than any baseline.

\begin{table}[t]
	\caption{RQ1. Detailed Comparison Results on Each Type of Faults Classified using \# of CC Fixing Statements.}
        \vspace{-6pt}
	{\small
		\begin{center}
			\renewcommand{\arraystretch}{1}
			\begin{tabular}{p{1cm}<{\centering}|p{1.33cm}<{\centering}|p{0.9cm}<{\centering}|p{0.7cm}<{\centering}|p{1.2cm}<{\centering}|p{1.2cm}<{\centering}}
				\hline
				\#Stmts & Metrics & CNN-FL & DeepFL & DeepRL4FL & \tool \\
				\hline
				\multirow{3}{*}{1 (199)}   & Hit-1@set     & 78 & 76 & 84 & 93 \\
							    		 & EXAM          & 0.09 & 0.09 & 0.08 & 0.06 \\
									% & EXAM\_AVG     & 0.09 & 0.09 & 0.08 & 0.06 \\
				\hline
				\multirow{4}{*}{2 (142)}  & Hit-1@set     & 67 & 64 & 70 & 75 \\
										& Hit-2@set     & 33 & 30 & 34 & 41 \\
									   	& EXAM          & 0.09 & 0.10 & 0.09 & 0.08 \\
			                     %	& EXAM\_AVG     & 0.22 & 0.23 & 0.19 & 0.15 \\
				\hline
				\multirow{5}{*}{3 (90)}  & Hit-1@set     & 46 & 44 & 47 & 51 \\
										& Hit-2@set     & 21 & 20 & 23 & 25\\
										& Hit-3@set     & 11 &10 & 13 & 21 \\
										& EXAM          & 0.11 & 0.12 & 0.11 & 0.09 \\
								%	& EXAM\_AVG     & 0.33 & 0.31 & 0.27 & 0.21 \\
				\hline
				\multirow{6}{*}{4 (78)}  & Hit-1@set     & 41 & 42 & 42 & 45 \\
										& Hit-2@set     &22 & 19 & 21 & 24 \\
										& Hit-3@set     & 9 & 7 & 8 & 12 \\
										& Hit-4@set     & 3 & 2 & 4 & 9 \\
										& EXAM          & 0.07 & 0.08 & 0.07 & 0.06 \\
								%	& EXAM\_AVG     & 0.39 & 0.44 & 0.38 & 0.34 \\
				\hline
				\multirow{7}{*}{5 (43)}  & Hit-1@set     & 15 & 14 & 16 & 18 \\
										& Hit-2@set     & 9 & 8 & 9 & 12 \\
										& Hit-3@set     & 6 & 5 & 6 & 7 \\
										& Hit-4@set     & 3 & 2 & 3 & 3 \\
										& Hit-5@set     & 1 & 1 & 1 & 1 \\
										& EXAM          & 0.08 & 0.09 & 0.08 & 0.08 \\
								%	& EXAM\_AVG     & 0.47 & 0.51 & 0.45 & 0.39 \\
				\hline
				\multirow{8}{*}{5+ (283)}  & Hit-1@set     & 85 & 91 & 93 & 105 \\
								 		& Hit-2@set     & 40 & 42 & 45 & 65 \\
										& Hit-3@set     & 31 & 34 & 37 & 42 \\
								 		& Hit-4@set     & 17 & 14 & 21 & 34 \\
										& Hit-5@set     & 4 & 3 & 5 & 8 \\
										& Hit-5+@set    & 1 & 2 & 3 & 3 \\
										& EXAM          & 0.11 & 0.11 & 0.10 & 0.09 \\
								%	& EXAM\_AVG     & 0.65 & 0.73 & 0.68 & 0.62 \\
				\hline
			\end{tabular}
			
			\label{fig:rq1-1}
		\end{center}
	}
\end{table}

%tips:

%1. The exam and exme\_avg score is the lower the better

%2. The hit@set is the higher the better

%2. \tool is the best performed approach



\begin{table}[t]
	\caption{RQ1. Comparison with Baselines in Hit-N@Top-$K$.}
        \vspace{-6pt}
	{\small
		\begin{center}
			\renewcommand{\arraystretch}{1}
			\begin{tabular}{p{1.5cm}<{\centering}|p{0.3cm}<{\centering}|p{0.3cm}<{\centering}|p{0.3cm}<{\centering}|p{0.2cm}<{\centering}|p{0.2cm}<{\centering}|p{0.3cm}<{\centering}|p{0.3cm}<{\centering}|p{0.3cm}<{\centering}|p{0.2cm}<{\centering}|p{0.2cm}<{\centering}|p{0.2cm}<{\centering}}
				\hline
				\multirow{2}{*}{Model}    & \multicolumn{5}{c|}{Hit-N@Top5}& \multicolumn{6}{c}{Hit-N@Top10}\\
				\cline{2-12}
											 &1&2&3&4&5&1&2&3&4&5&5+\\
				
				\hline
				CNN-FL      & 533 & 311 & 133 & 33 & 4 & 578 & 386 & 166 & 42 & 10 & 81 \\
				DeepFL		& 525 & 298 & 131 & 35 & 6 & 563 & 364 & 156 & 42 & 10 & 83 \\
				DeepRL4FL	& 586 & 339 & 159 & 32 & 9 & 623 & 407 & 186 & 48 & 13 & 92 \\
				\hline
				\tool       & 633 & 420 & 195 & 46 & 11& 690 & 470 & 217 & 51 & 13 & 94 \\
				\hline
			\end{tabular}
			
			\label{fig:rq1-2}
		\end{center}
	}
\end{table}

Table~\ref{fig:rq1-2} shows the comparison result in Hit-N@Top-$K$.
As seen, {\tool} can locate more faulty statements than any baseline
in Hit-N@Top-5 and Hit-N@Top-10. The results indicate that even in the
ranking setting, {\tool} locates more faulty statements than
any other baseline, especially dealing multiple faulty statements. For
example, {\tool} improves the best performing baseline DeepRL4RL by
23.9\% in Hit-2@Top-5, 22.6\% in Hit-3@Top-5, 43.8\% in Hit-4@Top-5,
and 22.2\% in Hit-5@Top-5, respectively. The Hit-N@Top-10 result has
the same trend as Hit-N@Top-5 in which {\tool} improves over the
baselines.

We did not compare with the spectrum-based and mutation-based FL
models since DeepRL4FL~\cite{icse21-fl} was shown to outperform
those approaches. Thus, our result shows that {\tool}
could improve over those spectrum-/mutation-based FL approaches.
