\subsubsection{Experimental Setup and Procedures.\\}
%\hspace{1cm}

{\bf RQ1. Comparison with DL-based FL Approaches.}

\vspace{1pt}
\underline{Baselines.}
Our tool aims to output a {\bf set} of CC fixing
statements for a bug. However, the existing Deep Learning-based FL
approaches can produce only the ranked {\bf lists} of suspicious
statements with scores. Thus, we chose as baselines the most
recent, state-of-the-art, DL-based, statement-level FL approaches:
%We compare {\tool} with the following state-of-the art Deep Learning
%(DL)-based FL approaches:
(1) \textbf{CNN-FL}~\cite{zhang2019cnn}; (2) {\bf
DeepFL}~\cite{DeepFL}; and (3) {\bf DeepRL4FL}~\cite{icse21-fl}; then,
we use the predicted, ranked list of statements as the output set.
For the {\em comparison in ranking with those ranking baselines}, we convert our
tool's result into a ranked list by ranking the statements in the
predicted set by the classification scores (i.e., before deriving the
final set). We also compare with the CC fixing-statement detection module
in {\bf DEAR}~\cite{icse22}, a multi-method/multi-statement APR tool.

%%%%
%The main goal of our tool is to output a {\bf set} of relevant faulty
%statements to a bug. However, the above compared baselines only
%produce the ranked {\bf lists} of suspicious statements with scores.

%To calculate \textbf{Hit-N@Set} for those baselines, if a predicted
%set from {\tool} has S statements, we consider the top-S predicted
%statements from a baseline model as the predicted set from that model
%for comparison.

%We chose them as the baselines because (1) they are the most recent
%DL-based FL approaches; and (2) they are applicable to statement-level
%FL.

%%%%-----

%a deep learning approach to automatically learn the most effective existing/latent features for precise fault localization.
%\item \textbf{DeepRL4FL~\cite{li2021fault}:} DeepRL4FL is a deep learning fault localization approach that locates the buggy code at the statement and method levels by treating FL as an image pattern recognition problem.

\vspace{1pt}
\underline{Procedures.}
We use the leave-one-out setting as in prior work~\cite{DeepFL,TraPT}
(i.e., testing on one bug and training on all other bugs).
%In the ***leave-one-out setting*** in FixLocator,
We also consider the order of the bugs in the same project via the
revision numbers. Specifically, for each buggy version $B$ of project
$P$ in Defects4J, all buggy versions from the other projects are first
included in the training data. Besides, we separate all the buggy
versions of the project $P$ into two groups: 1) one buggy version as
the test data for model prediction, and 2) all the buggy versions
of the same project $P$ that have occurred before the buggy version
$B$ are also included in the training data. If the latter group is
empty, only the buggy versions from the other projects are used for
training to predict for the current buggy version in $P$.


%We train all models using the leave-one-out setting following the
%previous studies~\cite{DeepFL, TraPT} to gain enough training data
%(i.e., testing on one bug, and using all other bugs for training).



%. Particularly, to test an approach on a bug, we use all the other
%bugs in Defects4J to train the approach.

%each bug $b_i$, we use all other bugs in the Defects4J dataset as the training dataset and then test the model on bug $b_i$. In this case, the training dataset is big enough for each bug. 
%There are many existing studies \cite{DeepFL, TraPT} also using the same setting. 

%\underline{ Parameter tuning and baseline setting.}

We tune all models using autoML~\cite{NNI} to find the best parameter
setting. We directly follow the baseline studies to select the
parameters that need to be tuned in the baselines.
%({\color{red}{you do not need to specify the parameters for baselines?
%you let autoML to select parameters?}}).
We tuned our model with the following key hyper-parameters to obtain
the best performance: (1) Epoch size (i.e., 100, 200, 300); (2) Batch
size (i.e., 64, 128, 256); (3) Learning rate (i.e., 0.001, 0.003,
0.005, 0.010); (4) Vector length of word representation and its output
(i.e., 150, 200, 250, 300); (5) The output channels of convolutional
layer (16, 32, 64,128); (6) The number of convolutional layers (3, 5,
7, 9).

%(5) Convolutional core size (i.e.,
%$3\times3,\:5\times5,\:7\times7,\:9\times9,$ $\:11\times11$); (6)
%The number of convolutional core (3, 5, 7, 9, and 11).

%We tune the parameters of {\tool} including epoch, batch size,
%learning rate, embedding length, hidden length, and the number of
%convolutional layers in GCN. The obtained best setting of {\tool} is
%{\color{red}{xxxxxx}}.

DeepFL was proposed for the method-level FL. For comparison, following
a prior study~\cite{icse21-fl}, we use only DeepFL's spectrum-based
and mutation-based features applicable to detect faulty statements.
%following the existing study DeepRL4FL \cite{li2021fault}.

%The other two baselines both can do the statement-level fault localization, so we direct use them.
\vspace{1pt}
\underline{Evaluation Metrics.}  
We use the following metrics for evaluation:

(1) {\bf Hit-N} measures the number of bugs that the predicted set
contains {\em at least} $N$ faulty statements (i.e., the predicted 
and oracle sets for a bug overlap at least $N$ statements regardless
of the sizes of both sets). Both precision and recall can be computed
from Hit-N.

(2) {\bf Hit-All} is the number of bugs in which the predicted set exactly
matches with the correct set in the oracle.

%The size of a predicted set can vary.

%Hit@N is the number of program versions for which a fault was found
%within the top N ranked statements that an approach correctly
%predicts at least $n$ statements This metric means that in the
%predicted set, .

%Tien removed EXAM Score
%(2) {\bf EXAM Score}~\cite{wong2008crosstab} is the percentage of
%statements that must be examined before the first faulty statement in
%the predicted list is encountered. We rank the statements based on the
%classification scores (i.e., before deriving the final set). If there
%is a tie in those scores, we assume half of those statements needed to
%be~examined. The lower EXAM score indicates less effort required
%in~FL.

(3) {\bf Hit-N@Top-$K$} is the number of bugs that the predicted list
of the top-$K$ statements contains at least $N$ faulty
statements. This metric is used when we compare the approaches in
ranking.




%faults that an approach correctly predicted at least $n$ statements.
%(3) {\bf $EXAM\_AVG$:} Because the $EXAM score$ only considers the
%effort of finding the first faulty statement. In this paper, we
%create a new evaluation metrics $EXAM\_AVERAGE$. For a bug that
%contains $m$ statements that need to be fixed, we calculate the $EXAM
%score = {EXAM@1, EXAM@2, ..., EXAM@m}$ for finding the $1, 2, ..., m$
%statements. And then we calculate the $EXAM\_AVERAGE
%= \frac{1}{m}\sum EXAM@m$


%To do so, for each bug $b_i$, we count the total number $k$ of statement in \tool's prediction results. And then, for the state-of-the-art fault localization approaches, we pick the top $k$ statements in the ranking list as a set to do the evaluation. In this setting, we have the following evaluation metrics:
%as the output while other state-of-the-art fault localization approaches often use the statement ranking lists with the ranking score for each statement as the output. To make a fair comparison. 
%We have two separate settings to make the comparison:

%{\bf List Prediction Setting:} Similarly, we can also transfer our prediction results into the statement ranking list. To do so, we collect the classification scores for each statement together as a list instead of setting a threshold to pick out the statement set as the final output. In this case, the evaluation metrics are also a little bit different:

%Tien removed
%The main goal of our tool is to output a {\bf set} of relevant faulty
%statements to a bug. However, the above compared baselines only
%produce the ranked {\bf lists} of suspicious statements with scores.
%
%To calculate \textbf{Hit-N@Set} for those baselines, if a predicted
%set from {\tool} has S statements, we consider the top-S predicted
%statements from a baseline model as the predicted set from that model
%for comparison.
%
%To calculate {\bf Hit-N@Top-$K$}, we convert our result into
%the ranked list (as in EXAM score) and keep the top-$K$.




%When comparing with the baselines, because the baselines all use the ranking list as their output, to make a fair comparison, we did the experiments on both the set prediction setting and the list prediction setting. Therefore, if \tool works better on both settings, it means \tool works better than baselines.

\vspace{3pt}
{\bf RQ2. Impact Analysis of Dual-Task Learning Model.}

\vspace{1pt}
\underline{Baselines.} To study the impact of dual-task learning, we built two variants of {\tool}: 
(1) {\em Statement-only model:} the method-level FL model
(\code{methFL}) is removed from {\tool} and only statement-level FL
(\code{stmtFL}) is kept for training. (2) {\em Cascading model:} in
this variant, dual-task learning is removed, and we cascade the
output of \code{methFL} directly to the input of \code{stmtFL}.

%It is changed to a two-tier model in which the statement-level fault
%localization model is dependent on the output of the method-level
%fault localization model.

%The sequential model contains both the method-level and the statement-level fault localization. However, it does not make these two models a dual-learning framework. Instead, it firstly runs the method-level fault localization to find the possible buggy methods and then runs the statement-level fault localization on top of that to find the buggy statement inside.

\vspace{1pt}
\underline{Procedures.}
The statement-only model has only the statement-level fault
localization. We ran it on all methods in the project to find the
faulty statements. We use the same training strategy and parameter
tuning as in RQ1.
%The parameters that need to be tuned in both two baselines are the
%same as in {\tool}.
We use Hit-N for evaluation.

\vspace{2pt}
{\bf RQ3. Sensitivity Analysis.}  We conduct ablation analysis to
evaluate the impact of different factors on the performance: every
{\em node feature}, {\em co-change relation}, and {\em the depth limit
on the stack trace and the execution trace}. Specifically, we set
{\tool} as the complete model, and each time we built a variant by
removing one key factor, and compared the results.
%between {\tool} and the variant to evaluate the impact of the factor.
Except for the removed factor, we keep the same setting
as in other experiments.
%We use \textbf{Hit-N@Set} and \textbf{EXAM Score} for evaluation.

\vspace{2pt}
{\bf RQ4. Evaluation on Python Projects.}
%
%{\bf FIXME.} To evaluate {\tool} on different programming languages, we ran it on the C/C++ benchmark ManyBugs \cite{manybugs} with 185 bugs from 15 different projects. We use \textbf{Hit-N@Set} and \textbf{EXAM Score} for evaluation.
To evaluate {\tool} on different programming languages, we ran it on the Python benchmark BugsInPy \cite{BugsInPy,widyasari2020bugsinpy} with 441 bugs from 17 different projects.

%We use \textbf{Hit-N@Set} and \textbf{EXAM Score} for evaluation.

%{\bf RQ5. Extrinsic Evaluation.} To evaluate {\tool}'s usefulness, we
%used it to replace the original CC fixing-location detection module in
%DEAR~\cite{icse22}, and compare the corresponding results.

\vspace{2pt}
{\bf RQ5. Extrinsic Evaluation.} To evaluate usefulness, we replaced
the original CC fixing-location module in DEAR~\cite{icse22} with
{\tool} to build a variant of DEAR, namely DEAR$^{FixL}$.  We also
added {\tool} and Ochiai FL~\cite{Ochiai} to CURE~\cite{cure-icse21}
to build two variants: CURE$^{FixL}$ (\tool + CURE) and CURE$^{Ochi}$
(Ochiai+CURE).
%as the one in~\cite{icse22}. 

%We compared the variants with the original~ones


%and compare the corresponding results.
