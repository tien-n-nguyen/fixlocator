\subsection{Key Ideas}
\label{sec:key-ideas}

%To address those challenges,
We propose {\tool}, a FL approach to locate all the CC fixing
locations (i.e., faulty statements) that need to be co-fixed
%modified in the same fix
for a bug. We have the following key ideas in both new model
and features:

{\bf Key Idea 1. [Dual-Task Learning for Fault Localization]} To avoid the
confounding effect in a naive solution of detecting faulty methods
first and then detecting faulty statements in those methods, we design
an approach that treats detecting dependent CC fixing locations as a
{\em dual-task learning} between them. First, the {\em method-level
  FL} model (\code{MethFL}) aims to learn the methods that need
to be modified in the same fix. Second, the {\em statement-level
  FL} model (\code{StmtFL}) aims to learn the co-fixing
statements regardless of whether they are in the same or different
methods.

Intuitively, \code{MethFL} and \code{StmtFL} are related to each
other, in which the results of one model can help the other. We refer
to this relation as {\em duality}, which can provide some useful
constraints for {\tool} to learn dependent CC fixing locations.
%
We conjecture that the joint training of the two models can improve
the performance of both models, when we leverage the constraints of
this duality in term of shared representations. For example, if two
statements in two different methods $m_1$ and $m_2$ were observed to
be changed in the same fix, then it should help the model learn that
$m_1$ and $m_2$ were also changed together to fix the bug.  If two
methods were observed to be fixed together, then some of their
statements were changed in the same fix as well. In our model, we
jointly train \code{MethFL} and \code{StmtFL} with the models'
parameter soft-sharing to exploit their relation. We use a mechanism
called {\em cross-stitch unit}~\cite{misra2016cross} to learn a linear
combination of the input features from two models to {\em enable
  the propagation of the impact of \code{MethFL} on \code{StmtFL} and
  vice~versa}.

%We also add an attention mechanism in the two models to help emphasize
%on the key features.


%Therefore, the third component is the {\em localization model} with
%the main goal of deriving the CC fixing locations. Another task of
%this component is to train the MethFL and StmtFL simultaneously to
%explot the duality. Specifically, we apply a probabilistic correlation
%as a regualization term in the foss function in the join training. We
%also design a novel constraint about the attention mechanism in two
%models.

%two dual regularization terms to constrain the duality of the two
%models, which are enlightened by the probabilistic correlation and the
%symmetry of attention weights between two models.




%{\bf Key Idea 3. [Multi-task Learning Fault Localization]} When doing the fault localization, there are often two levels of fault localization that we can do: the statement-level fault localization and the method-level fault localization. More detailed fault localization can help the developers to find the bugs easier. So for\tool, we regard the statement-level fault localization as our primary goal. However, the method fault localization is still helpful because the method-level fault localization often has higher accuracy and can help reduce the biases. 

%To make the method-level fault localization helpful when doing the statement-level fault localization, in \tool, we build a multi-task learning framework to do the statement-level fault localization. To be more specific, we have two separate models for the two levels of fault localization. The first model is to do the statement-level fault localization as we mentioned in key idea 2. And the second model is to do the method-level fault localization. In this model, we take the method pairs as input and train the model to predict if the method pairs are co-changes or not. During the training process, the parameters between the first and second models are softly shared to build the multi-task learning framework. We will introduce the details in the approach section.

{\bf Key Idea 2. [Co-Change Representation Learning in Fault
    Localization]} In detecting CC fixing locations, in addition to a
new dual-task learning model in key idea 1, we use a new feature:
%{\em co-change information}, i.e., the statements/methods that were
%changed together.
{\em co-change information} among statements/methods, which has never
explored in prior fault localization research. The rationale
%for considering general co-changes
is that the co-changed entities in the past might become the ones that
will be fixed together for a bug in the future.
%
The co-changed statements/methods in the same commit are used to train
the models. We also encode the {\em co-fixed statements/methods in
the same fixes}.

%Just as we mentioned in observation 1, co-change is very common when
%we want to fix a bug or do an enhancement on the code. To catch the
%co-change information, we first collect the commits that change the
%source code. And for each commit, we mark the statements that changed
%together as the co-change. By collecting all co-changes from the
%commits in the project, we have a large co-change history
%dataset. Then, to analyze the code relationship, we build a link
%between the statements been changed together. To make the co-change
%information useful, we add these edges into the other graph to do the
%graph modeling. That is our second key idea.



{\bf Key Idea 3. [Graph Modeling for Dependencies among
    Statements/Methods]} The statements/methods that need to be fixed
together are interdependent via several dependencies. Thus, we use
Graph-based Convolution Network (GCN)~\cite{li2019gcn} to model
different types of dependencies among statements/methods,
e.g., data and control dependencies in a program dependence graph
(PDG), execution traces, stack traces, etc.
%For convenience,
We encode the {\em co-change/co-fix relations}
%in the same commit or fix} among the statements
into graph representations with different types of edges
representing different relations. The GCN model enables nodes'
and edges' attributes and learns to classify the nodes as buggy
or~not.

%{\bf Key Idea 2. [Multi-edge-types Graph Modeling]}

%As mentioned in observation 2, one bug can be involved in multiple
%methods, and for each method, there can be more than one statement
%related to the bug. To analysis the relationships between the
%statements that in the same method, the graph modeling between the
%source code such as program dependency graph (PDG), execution paths
%(EP), and co-change relationships (CCR) is the way that we thought to
%be suitable. To avoid overlapping, we regard the PDG as the based
%graph, add the EP and CCR edges into the graph, and use the combined
%graph to analyze the relationship.

%However, the different types of edges in one graph cannot be regarded as the same type. To specify the differences among the four different types of edges (PDG contains data dependency and control dependency, two types of edges), we use an advanced GCN \cite{li2019gcn} that takes both node and edge attributes as inputs to learn the node classification. Because it accepts edge attributes, we give the different types of edges with different labels as input to specify their differences.






