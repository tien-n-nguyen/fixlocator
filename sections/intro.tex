\section{Introduction}

Detecting and fixing software defects is crucial for a software
development process. To reduce the efforts from developers in that
process, several {\em fault localization} (FL)
approaches~\cite{fl-survey} have been introduced to help localize the
source of the fault that needs to be fixed. The input of an FL model
is the execution of a test suite, in which some of the test cases are
passing or failing ones. Specifically, the key input is the {\em code
  coverage matrix} in which the rows and columns correspond to the
statements and test cases, respectively.  Each cell is assigned with
the value of 1 if the respective statement is executed in the
respective test case, and with the value of 0, otherwise.  An FL model
uses such information to identify the list of {\em suspicious lines of
  code} that are ranked based on their associated {\em suspiciousness
  scores}~\cite{fl-survey}. In recent advanced FL, several approaches
also support fault localization at method
level~\cite{DeepFL,icse21-fl}. 



%In the FL problem, given the execution of test cases, an FL tool
%identifies the set of {\em suspicious lines of code} with their
%associated suspiciousness scores~\cite{fl-survey}.  The key input of
%an FL tool is the {\em code coverage matrix} in which the rows and
%columns correspond to the source code statements and test cases,
%respectively.  Each cell is assigned with the value of 1 if the
%respective statement is executed in the respective test case, and with
%the value of 0, otherwise. In recent FL, several researchers also
%advocate for fault localization at method level~\cite{DeepFL}. FL at
%both levels are useful for developers.

The FL approaches can be broadly divided into the following
categories: {\em spectrum-based fault localization} (SBFL)
approaches~\cite{Ochiai,jones2001visualization,keller2017critical},
{\em mutation-based fault localization} (MBFL)
approaches~\cite{MUSE,papadakis2012using,Metallaxis}, and {\em machine
  learning (ML)} and {\em deep learning (DL)}~\cite{DeepFL,icse21-fl}.
For SBFL approaches, the key idea is that a line covered more in the
failing test cases than in the passing ones is more suspicious than a
line executed more in the passing ones.
%
To improve SBFL, MBFL
approaches~\cite{MUSE,papadakis2012using,Metallaxis} enhance the code
coverage information by modifying a statement with mutation operators,
and then collecting code coverages when executing the mutated programs
with the test cases. The MBFL approaches apply suspiciousness score
formulas in the same manner as in SBFL approaches on the matrix for
each original statement and its mutated code.
%
ML and DL-based FL approaches explore the code coverage matrix and
apply different neural network models for fault localization.


%{\em Spectrum-based fault localization} (SBFL)
%approaches~\cite{Ochiai,jones2001visualization,keller2017critical}
%take the recorded lines of code that were covered by each of the given
%test cases, and assigned each line of code a suspiciousness score
%based on the code coverage matrix. Despite using different
%formulas to compute that score, the idea is that a line covered more
%in the failing test cases than in the passing ones is more suspicious
%than a line executed more in the passing ones. A key drawback of those
%approaches is that the same score is given to the lines that have been
%executed in both failing and passing test cases. An example is the
%statements that are part of a block statement and executed at the
%same nested level. Another example is the conditions of the
%condition statements, e.g., \code{if}, \code{while}, \code{do},
%and \code{switch}. 

%To improve SBFL, {\em mutation-based fault localization} (MBFL)
%approaches~\cite{MUSE,papadakis2012using,Metallaxis}
%enhance the code coverage information by modifying a statement with
%mutation operators, and then collecting code coverages when executing
%the mutated programs with the test cases. They apply suspiciousness
%score formulas in the same manner as the spectrum-based FL approaches on
%the code coverage matrix for each original statement and its mutated
%ones. Despite the improvement, MBFL are not effective for the bugs
%that require the fixes that are more complex than a mutation
%(Section~\ref{motivexample}).

%{\em Machine learning (ML)} and {\em deep learning (DL)}
%have been used in fault localization. DeepFL~\cite{DeepFL}
%computes for each faulty method a vector with +200 scores in which
%each score is computed via a specific feature, e.g., a spectrum-based
%or mutation-based formula, or a code complexity metric. Despite its
%success, the accuracy of DeepFL is still limited. A reason could be
%that it uses various calculated scores from different formulas as a
%proxy to learn the suspiciousness of a faulty element, instead of
%fully exploiting the code coverage. Some formulas, such
%as the spectrum- and mutation-based formulas, inherently suffer from
%the issues as explained earlier with the statements covered by
%both failing and passing test cases.

Despite their successes, the state-of-the-art FL approaches still do
not support locating all dependent fixing locations that need to be repaired
at the same time in the same fix. In real-world software development,
there are several bugs that require a fix to multiple lines of code in
one or multiple hunks in the same or different methods. {\em The
  fixing changes to those lines of code are dependent to one another
  and need to be made in the same fix for the program to pass the test
  cases}. For those bugs, applying the fixing change to one statement
at a time will not make the program pass the test case after the
change to one statement.
%
This capability to detect the fixing locations of the co-changes in a
fix for a bug (let us call it {\em Co-change Fixing Locations} ({\em
  CC Fix Locations})) is important for both the manual process of bug
fixing as well as the automated process of program repair. For
manual process, such capability will save effort and time for
developers in locating all the buggy statements that need to be fixed
at the same time. For automated program repair (APR), such capability
will enable an APR model to correctly and completely make the changes
to fix a bug.

From the ranked list of suspicious statements returned from an
existing FL model, a naive approach to detect CC Fix Locations would
be to take the top $k$ statements in that list and to consider them as
to be fixed together. This solution might not be effective
because the mechanisms used in the state-of-the-art FL approaches have
never considered the co-change nature of those fixes. Our empirical
evaluation also confirmed that (Section~\ref{eval:sec}).

Detecting all the fixing locations at multiple statements in
potentially multiple methods is challenging. A naive solution would be
detecting the potential methods that need to be fixed together and
then detecting potential statements that need to be changed together
in each of those methods. However, doing so will create a comfounding
effect from the inaccuracy of the detection of buggy methods to the
detection of the buggy statements.

We propose {\tool}, a fault localization approach for buggy
statements/methods that can locate multiple buggy statements in
possible multiple buggy methods. To avoid the confounding effect in
the above naive solution, our solution treats this problem as a {\em
  dual learning} task with two models. First, the {\em method-level
  FL} (\code{MethFL}) model learns the methods that need to be
modified in the same fix. Second, the {\em statement-level FL}
(\code{StmtFL}) model learns the co-fixing statements in the same or
different methods. The intuition is that they are closely related,
which we refer to as {\em duality}. Learning for a model can benefit
for the other and vice versa. If two statements in two methods are
fixed together (co-fixed) for a bug, those methods are also
co-fixed. If two methods are co-fixed, some of their statements
are also co-fixed. Exploring this~duality can provide useful
constraints for our model to detect CC fixing locations. Thus,
instead of cascading them, we train them simultaneously with
soft-sharing the parameters of the models to exploit this
duality. We apply a probabilistic correlation as a regulization term
in the loss function in the join training. We also design a novel
constraint about the attention mechanism in the two models.

In addition to the new dual learning model solution, we also explore a
novel feature for this CC fixing location problem: co-change
statements, which are the ones changed together. The rationale to
consider general co-changes is that those co-changed statements in the
past might become the statements that will be fixed together in the
future. Finally, because the co-fixed statements are often dependent
to one another, we use Graph-based Convolution Network
(GCN)~\cite{li2019gcn} to integrate different types of program
dependencies among the statements including program dependence graph
(PDG), and execution path (EP). We also encode test coverage and the
co-change statements in the graph. The GCN model will learn and class
the bugginess of the statements.

%The statements that need to be fixed together are often dependent on
%one another with program dependencies. Thus, we use Graph-based
%Convolution Network (GCN)~\cite{li2019gcn} to model different types of
%dependencies among the statements. Thus, we encode both the program
%dependence graph (PDG) and execution path (EP) in our graph modeling.
%For convenience, we also encode the co-change relations among the
%statements into our general graph representations with different types
%of edges representing different relations. The GCN model enables both
%nodes’ and edges’ attributes and learns to classify the nodes.

{\bf FIXME.} We conducted several experiments to evaluate {\tool} on
Defects4J benchmark~\cite{defects4j}. Our empirical results show that
\tool locates 245 faults and 71 faults at the method level and the
statement level, respectively, using only top-1 candidate (i.e., the
first ranked element is faulty). It can improve the top-1 results of
the state-of-the-art \textit{statement-level} FL baselines by 317.7\%,
273.7\%, 173.1\%, 195.8\%, and 491.7\% when comparing with
Ochiai~\cite{Ochiai}, Dstar~\cite{DStar}, Muse~\cite{MUSE},
Metallaxis~\cite{Metallaxis}, and RBF-Neural-Network-based FL
(RBF)~\cite{RBF_Neural_Network}, respectively.  {\tool} also improves
the top-1 results of the existing \textit{method-level} FL baselines,
MULTRIC~\cite{MULTRIC}, FLUCCS~\cite{FLUCCS}, TraPT~\cite{TraPT}, and
DeepFL~\cite{DeepFL}, by 206.3\%, 53.1\%, 57.1\%, and 15.0\%,
respectively. Our results show that three sources of information in
{\tool} positively contribute to its high accuracy. We also evaluated
{\tool} on ManyBugs~\cite{LeGoues15tse}, a~ben\-chmark of C code with
9 projects. The results are~consistent with the ones on Java code.
{\tool} localizes 27 faulty statements and 98 faulty methods using
only top-1 results.

The contributions of this paper are listed as follows:

{\bf 1. {\tool}: Novel DL-based fault localization approach} that
derives the co-change fixing locations for a bug. Our idea is
to treat such problem as a dual learning task with the joint training
of the method-level and statement-level co-fixing learning models.

{\bf 2. Novel graph-based representation learning with co-change
  statements.} Our graph-based representation learning with GCN
and the novel type of features in co-change statements enables
the dual-task models learn derive co-change fixing locations.

{\bf 3. Extensive empirical evaluation.} We evaluated {\tool} against
the most recent FL models to show our model's better performance. Our
replication package is available at~\cite{FixLocator2022}.

