\section{Related Work}
%we need to talk about the newly fse lingming's paper, code not available, and also their results on benchmark is even lower than deepFL.
%has been intensively studied in the literature, e.g., Ochiai~\cite{abreu2006evaluation} and Jaccard~\cite{abreu2007accuracy}.

Several types of techniques have been proposed to locate faulty
statements/methods. However, none of the existing FL approaches detect
{\em CC fixing locations}.
%as in {\tool}.
A related work is DEAR~\cite{icse22}, which uses a combination
of BERT and data flows to locate CC statements.
%which has two components: CC fixing location detection and auto-fixing.
%Our experiment (Section~\ref{sec:eval}) shows that {\tool} performed
%better than DEAR's CC fixing location detection component and helps
%DEAR improve bug-fixing.
Hercules APR tool~\cite{hercules-icse19} can detect multiple buggy
hunks of code. It can detect only the buggy hunks with similar
statements (replicated fixes), while our tool detects general CC
fixing locations. In comparison, {\tool} and Hercules detect 26 and 15
multi-hunk bugs respectively among 395 bugs in
Defects4J-v1.2~\cite{hercules-icse19}. Hercules is not
available to run on Defects4J-v2.0 (used in our work).

The Spectrum-based Fault Localization
(SBFL)~\cite{zhang2011localizing, abreu2007accuracy,
  jones2005empirical, abreu2006evaluation, naish2011model,
  wong2007effective, liblit-pldi05, lucia2014extended} and
Mutation-based Fault Localization (MBFL)~\cite{Metallaxis, MUSE,
  zhang2013injecting, budd1981mutation, zhang2010test, musco2017large}
have been proposed for statement-level FL. Their key limitations are
that they cannot differentiate the statements with the same scores or
cannot have effective mutators to catch a complex fault.
%that is complex than a mutator. Recently,
Recently, among the learning-based FL models, learning-to-rank FL
approaches~\cite{MULTRIC,TraPT,b2016learning,sohn2017fluccs} aim to
locate faulty methods.
%they all at method level fault localization
%A recent work combining statistical FL with casual inference
%techniques~\cite{kuccuk2021improving} has been developed for
%statement-level FL.
Statistical FL has been combined with casual inference for
statement-level FL~\cite{kuccuk2021improving}. However, all of
those models do not aim to locate multiple CC fixing statements as {\tool}.
%However, all of the above approaches are designed to perform a binary
%classification on statements or methods to rank the suspicious
%candidates. Their goal limits their ability of locating multiple CC
%fixing statements.

%With advances in ML, researchers have used deep learning
%for FL.
Machine learning has also been used for FL.  Early neural
network-based FL~\cite{zheng2016fault, briand2007using, zhang2017deep,
  wong2009bp} mainly use test coverage data. A limitation is that they
cannot distinguish elements accidentally executed by failed tests and
the actual faulty elements~\cite{TraPT}.
%
Deep learning-based approaches, GRACE~\cite{lou2021boosting},
DeepFL~\cite{DeepFL}, CNNFL~\cite{zhang2019cnn},
DeepRL4FL~\cite{icse21-fl} achieve better
results. GRACE~\cite{lou2021boosting} proposes a new graph
representation for a method and learns to rank the faulty methods.  In
contrast, {\tool} is aimed to locate multiple CC fixing
statements in a fix for a fault. DeepFL and DeepRL4FL can outperform the
learning-based and early neural networks FL techniques, such as
MULTRIC~\cite{MULTRIC}, TrapT~\cite{TraPT}, and
Fluccs~\cite{sohn2017fluccs}. In our empirical evaluation, we 
showed that {\tool} can outperform the compared baselines in detecting
CC fixing statements.


%Lingming's paper GRACE has code now, but we won't have time to implement it and also it is designed for method ranking, our approach is proposed for statement-level. Their model is designed for method-level fault localization, aiming to provide rankings of faulty methods, which is different from our work.

%also talk about the spectrum and mutation work, also the predicates-based approaches (icse'21 newly work).


%\textbf{CNN-FL~\cite{zhang2019cnn}}; (2) {\bf DeepFL~\cite{DeepFL}} ; and (3) {\bf DeepRL4FL \cite{icse21-fl}}.








%============================= below is from DeepRl4Fl=========================

\iffalse

The Spectrum-based Fault Localization
(SBFL), e.g.,~\cite{Ochiai,abreu2007accuracy,
	jones2005empirical,keller2017critical, liblit2005scalable,
	lucia2014extended,naish2011model, wong2007effective,
	zhang2011localizing}, has been intensively studied in the
literature.  Tarantula \cite{jones2001visualization}, SBI
\cite{liblit2005scalable}, Ochiai \cite{Ochiai} and Jaccard
\cite{abreu2007accuracy}, they share the same basic insight, i.e.,
code elements mainly executed by failed tests are more suspicious.
%Although various SBFL techniques have been proposed, e.g., Tarantula \cite{jones2001visualization}, SBI \cite{liblit2005scalable}, Ochiai \cite{abreu2006evaluation} and Jaccard \cite{abreu2007accuracy}, they share the same basic insight, i.e., code elements mainly executed by failed tests are more suspicious.
%The input of SBFL is the coverage information of all tests and the output is a ranked list of code elements (e.g., statements or methods) according to their descending order of suspiciousness values calculated by specific formula.
The Mutation-based Fault Localization (MBFL), e.g.,~\cite{budd1981mutation,MUSE,musco2017large,zhang2010test,
	zhang2013injecting},
aims to additionally consider mutated code in fault
localization.
%since code elements covered by failed/passed tests may
%not have impact on the corresponding test outcomes.
The examples of MBFL are Metallaxis \cite{papadakis2012using,
	Metallaxis} and MUSE \cite{MUSE}.
%Mutation-based Fault Localization (MBFL), e.g.,~\cite{moon2014ask, zhang2013injecting,budd1981mutation, zhang2010test}, aims to additionally consider impact information for fault localization. Since code elements covered by failed/passed tests may not have any impact on the corresponding test outcomes, e.g., Metallaxis \cite{papadakis2012using, papadakis2015metallaxis} and MUSE \cite{moon2014ask}.
%
%typical MBFL techniques
%use mutation testing \cite{budd1981mutation, zhang2010test, musco2017large} to simulate the impact of each
%code element for more precise fault localization, e.g., Metallaxis \cite{papadakis2012using, papadakis2015metallaxis} and MUSE \cite{moon2014ask}.
%The first general MBFL technique, Metallaxis [24, 26] is based on the
%following intuition: if one mutant has impacts on failed tests (e.g.,
%the tests outcomes change after mutation), its corresponding code
%element may have caused the test failures
\fi
