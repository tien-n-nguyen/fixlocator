\subsubsection{Experiment Setup and Procedure}
\hspace{1cm}

{\bf RQ1. Comparison with DL-based FL Approaches.}

\underline{Baselines.} We compare {\tool} with the following state-of-the art Deep Learning (DL)-based FL approaches: (1) \textbf{CNN-FL~\cite{zhang2019cnn}}; (2) {\bf DeepFL~\cite{DeepFL}} ; and (3) {\bf DeepRL4FL \cite{icse21-fl}}.
We chose the above approaches as the comparable baselines due to the following reasons: (1) They are the most recent deep learning based fault localization approaches; and (2)Most importantly, they are applicable to statement-level fault localization with little or no modification. 

%a deep learning approach to automatically learn the most effective existing/latent features for precise fault localization.
%\item \textbf{DeepRL4FL~\cite{li2021fault}:} DeepRL4FL is a deep learning fault localization approach that locates the buggy code at the statement and method levels by treating FL as an image pattern recognition problem.

\underline{Procedures.}
We train all models using the leave-one-out setting following the
previous studies~\cite{DeepFL, TraPT} to gain enough training data
(i.e., testing on one bug, and using all other bugs for
training).

%. Particularly, to test an approach on a bug, we use all the other
%bugs in Defects4J to train the approach.

%each bug $b_i$, we use all other bugs in the Defects4J dataset as the training dataset and then test the model on bug $b_i$. In this case, the training dataset is big enough for each bug. 
%There are many existing studies \cite{DeepFL, TraPT} also using the same setting. 

%\underline{ Parameter tuning and baseline setting.}

We tune all models using autoML~\cite{NNI} to find the best parameter
setting. We directly follow the baseline studies to select the
parameters that need to be tuned in the baselines.
%({\color{red}{you do not need to specify the parameters for baselines?
%you let autoML to select parameters?}}).
We tuned our model with the following key hyper-parameters to obtain
the best performance: (1) Epoch size (i.e., 100, 200, 300); (2) Batch
size (i.e., 64, 128, 256); (3) Learning rate (i.e., 0.001, 0.003,
0.005, 0.010); (4) Vector length of word representation and its output
(i.e., 150, 200, 250, 300); (5) Convolutional core size (i.e.,
$3\times3,\:5\times5,\:7\times7,\:9\times9,$ $\:11\times11$); (6)
The number of convolutional core (3, 5, 7, 9, and 11).

%We tune the parameters of {\tool} including epoch, batch size,
%learning rate, embedding length, hidden length, and the number of
%convolutional layers in GCN. The obtained best setting of {\tool} is
%{\color{red}{xxxxxx}}.

DeepFL was proposed for the method-level FL. For comparison, following
existing study~\cite{icse21-fl}, we use only DeepFL's spectrum-based
and mutation-based features applicable to detect faulty statements.
%following the existing study DeepRL4FL \cite{li2021fault}.

%The other two baselines both can do the statement-level fault localization, so we direct use them.

\underline{Evaluation Metrics.}  
%{\bf Set Prediction Setting:} In this setting, we transfer the output from the state-of-the-art fault localization approaches to a set of statements as the output. 
We use the following metrics for evaluation:

(1) {\bf Hit-N@Set} measures the number of times that the predicted
set from {\tool} contains at least N faulty statements. The size of a
predicted set can vary.

%Hit@N is the number of program versions for which a fault was found
%within the top N ranked statements that an approach correctly
%predicts at least $n$ statements This metric means that in the
%predicted set, .

(2) {\bf EXAM Score~\cite{wong2008crosstab}} is the percentage of
statements that one must examine before finding the first
faulty statement in the predicted set. We rank the statements based on
the classification scores (i.e., before deriving the final set).
%Specifically, we collect the classification scores of statements from
%{\tool} and use them to rank the statements before spinning out the
%final predicated sets, then pick the Top-K ranked statements for
%comparison with baselines.
If there is a tie in those scores, we assume half of
those statements needed to be~examined. The lower EXAM score indicates
less effort required in~FL.

(3) {\bf Hit-N@Top-$K$} is the number of times that the top-$K$
statements contain at least N faulty statements.

%faults that an approach correctly predicted at least $n$ statements.
%(3) {\bf $EXAM\_AVG$:} Because the $EXAM score$ only considers the
%effort of finding the first faulty statement. In this paper, we
%create a new evaluation metrics $EXAM\_AVERAGE$. For a bug that
%contains $m$ statements that need to be fixed, we calculate the $EXAM
%score = {EXAM@1, EXAM@2, ..., EXAM@m}$ for finding the $1, 2, ..., m$
%statements. And then we calculate the $EXAM\_AVERAGE
%= \frac{1}{m}\sum EXAM@m$


%To do so, for each bug $b_i$, we count the total number $k$ of statement in \tool's prediction results. And then, for the state-of-the-art fault localization approaches, we pick the top $k$ statements in the ranking list as a set to do the evaluation. In this setting, we have the following evaluation metrics:
%as the output while other state-of-the-art fault localization approaches often use the statement ranking lists with the ranking score for each statement as the output. To make a fair comparison. 
%We have two separate settings to make the comparison:

%{\bf List Prediction Setting:} Similarly, we can also transfer our prediction results into the statement ranking list. To do so, we collect the classification scores for each statement together as a list instead of setting a threshold to pick out the statement set as the final output. In this case, the evaluation metrics are also a little bit different:

The main goal of our tool is to output a {\bf set} of relevant faulty
statements to a bug. However, the above compared baselines only
produce the ranked {\bf lists} of suspicious statements with scores.
%
To calculate \textbf{Hit-N@Set} for those baselines, if a predicted
set from {\tool} has S statements, we consider the top-S predicted
statements from a baseline model as the predicted set from that model
for comparison.
%
To calculate {\bf Hit-N@Top-$K$}, we convert our result into
the ranked list as explained and keep the top-$K$.




%When comparing with the baselines, because the baselines all use the ranking list as their output, to make a fair comparison, we did the experiments on both the set prediction setting and the list prediction setting. Therefore, if \tool works better on both settings, it means \tool works better than baselines.

{\bf RQ2. Impact Analysis of Dual-learning Model.}

\underline{Baselines.} To study the impact of dual-learning, we built two variants of {\tool}: 
(1) \textbf{only-statement-model:} The method-level FL model
(\code{methFL}) is removed from {\tool} and only statement-level FL
(\code{stmtFL}) is kept for training. (2) \textbf{Cascading model:} In
this variant, the dual-learning is removed in which the output
of \code{methFL} is connected to the input of \code{stmtFL} in a
cascading manner.

%It is changed to a two-tier model in which the statement-level fault
%localization model is dependent on the output of the method-level
%fault localization model.

%The sequential model contains both the method-level and the statement-level fault localization. However, it does not make these two models a dual-learning framework. Instead, it firstly runs the method-level fault localization to find the possible buggy methods and then runs the statement-level fault localization on top of that to find the buggy statement inside.

\underline{Procedures.}
The only-statement-model baseline only has the statement-level fault
localization. We ran it on all methods in the project to find the
faulty statements. We use the same training strategy as in RQ1.  The
parameters that need to be tuned in both two baselines are the same as
in {\tool}. We use \textbf{Hit-N@Set} and \textbf{EXAM Score} for
evaluation.

{\bf RQ3. Sensitivity Analysis.}  In this RQ, we conduct ablation
analysis to evaluate the impact of the following factors on the
performance of {\tool}: each {\bf node feature}, {\bf co-change
relation}, and {\bf the depth limit on the stack trace and the
execution trace}. Specifically, we set \tool as the complete model,
and each time we built a variant by removing one key factor and
compared the differences between {\tool} and the variant to evaluate
the impact of the factor. Except for the removed factor, we keep the
same setting and parameters for all experiments. We
use \textbf{Hit-N@Set} and \textbf{EXAM Score} for evaluation.

{\bf RQ4. Evaluation on Python Projects.}
%
{\bf FIXME.} To evaluate {\tool} on different programming languages, we ran it on
the C/C++ benchmark ManyBugs \cite{manybugs} with 185 bugs from 15
different projects. We use \textbf{Hit-N@Set} and \textbf{EXAM Score}
for evaluation.
