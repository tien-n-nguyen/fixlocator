\subsubsection{Experiment Setup and Procedure}
\hspace{1cm}

{\bf RQ1. Comparison with State-of-the Art Approaches.}

\underline{Baselines.} We compare {\tool} with the following FL approaches: (1) \textbf{CNN-FL~\cite{zhang2019cnn}}; (2) \textbf{DeepFL~\cite{DeepFL}} ; and (3) \textbf{DeepRL4FL~\cite{li2021fault}}

%a deep learning approach to automatically learn the most effective existing/latent features for precise fault localization.
%\item \textbf{DeepRL4FL~\cite{li2021fault}:} DeepRL4FL is a deep learning fault localization approach that locates the buggy code at the statement and method levels by treating FL as an image pattern recognition problem.


In this RQ, we train \tool on the leave-one-out setting to increase the training dataset size. To be more detailed, for each bug $b_i$, we use all other bugs in the Defects4J dataset as the training dataset and then test the model on bug $b_i$. In this case, the training dataset is big enough for each bug. There are many existing studies \cite{DeepFL, TraPT} also using the same setting. 

When comparing with the baselines, because the baselines all use the ranking list as their output, to make a fair comparison, we did the experiments on both the set prediction setting and the list prediction setting. Therefore, if \tool works better on both settings, it means \tool works better than baselines.

\underline{ Parameter tuning and baseline setting.} We tune all baselines, and \tool using autoML technique \cite{NNI} to find the best parameter setting. We directly follow the baseline studies to select the parameters that need to be tuned in three baselines. As for \tool, we tune the parameters including {\bf epoch}, {\bf batch size}, {\bf learning rate}, {\bf embedding length}, {\bf hidden length}, and {\it the number of convolutional layers in GCN}.

Because the DeepFL baseline is only on the method level, to make it comparable, we only use DeepFL's spectrum-based and mutation-based features applicable to detect buggy statements. This process follows the existing study DeepRL4FL \cite{li2021fault}. The other two baselines both can do the statement-level fault localization, so we direct use them.

\underline{Evaluation Metrics.}  
%{\bf Set Prediction Setting:} In this setting, we transfer the output from the state-of-the-art fault localization approaches to a set of statements as the output. 
We use the following metrics to evaluate approaches: 
(1) {\bf Hit-N@Set} measures the number of times the predicted sets contain at least N statements, where N$\le$M and M is the number of faulty statements for a fault. The size of a predicated set from {\tool} can vary. %Hit@N is the number of program versions for which a fault was found within the top N ranked statements that an approach correctly predicts at least $n$ statements This metric means that in the predicted set, . 
(2) {\bf EXAM Score~\cite{wong2008crosstab}} is the percentage of program statements a developer must manually examine before finding the first faulty statement in the predicated sets. We rank the statements in a predicated set. If there are ties in the scores of statements, we assumed half of the tied statements that are needed to be examined. 
(3) {\bf Hit-N@Top-K} is the number of times at least N faulty statements appear among the Top-K ranked statements. %faults that an approach correctly predicted at least $n$ statements.
%(3) {\bf $EXAM\_AVG$:} 
%Because the $EXAM score$ only considers the effort of finding the first faulty statement. In this paper, we create a new evaluation metrics $EXAM\_AVERAGE$. For a bug that contains $m$ statements that need to be fixed, we calculate the $EXAM score = {EXAM@1, EXAM@2, ..., EXAM@m}$ for finding the $1, 2, ..., m$ statements. And then we calculate the $EXAM\_AVERAGE = \frac{1}{m}\sum EXAM@m$


%To do so, for each bug $b_i$, we count the total number $k$ of statement in \tool's prediction results. And then, for the state-of-the-art fault localization approaches, we pick the top $k$ statements in the ranking list as a set to do the evaluation. In this setting, we have the following evaluation metrics:
%as the output while other state-of-the-art fault localization approaches often use the statement ranking lists with the ranking score for each statement as the output. To make a fair comparison. 
%We have two separate settings to make the comparison:

%{\bf List Prediction Setting:} Similarly, we can also transfer our prediction results into the statement ranking list. To do so, we collect the classification scores for each statement together as a list instead of setting a threshold to pick out the statement set as the final output. In this case, the evaluation metrics are also a little bit different:

The main goal of {\tool} is to output a set of relevant faulty statements to a fault. However, the above compared baselines only produce lists of suspicious statements with scores. 
To calculate \textbf{Hit-N@Set}, if a predicated set has S statements, we use the Top-S predicated statements from baselines as a set for comparison with {\tool}. To calculate {\bf Hit-N@Top-K}, we collect the classification scores of statements and use them to rank the statements before spinning out the final predicated sets, then pick the Top-K ranked statements for comparison with baselines.




{\bf RQ2. Impact Analysis of Dual-learning Model.}

\underline{Baselines.} To study the impact of dual-learning, we built two other variants of {\tool}: (1) \textbf{only-statement-model:} The only layer model here only contains the statement-level fault localization part of \tool without the dual-learning structure. (2) \textbf{Sequential-based model:} The sequential model contains both the method-level and the statement-level fault localization. However, it does not make these two models a dual-learning framework. Instead, it firstly runs the method-level fault localization to find the possible buggy methods and then runs the statement-level fault localization on top of that to find the buggy statement inside.

In this RQ, we use the same training strategy as in RQ 1. And for one layer model baseline, because this baseline only has the statement-level fault localization, we run it on all methods in the project to find the buggy statements. Because all baselines in this RQ use set of statements as the final output, we only do the set perdition setting in this RQ.

\underline{ Parameter tuning and baseline setting.} We tune all these two baselines and \tool also using the autoML technique. We tune the parameters of the \tool in the same way as RQ1. And the parameters that need to be tuned in both two baselines are the same as \tool. Also we use the same metrics as the ones in RQ1.

{\bf RQ3. Sensitivity Analysis.}

In this RQ, we do the experiments to evaluate the impact of each {\bf node features}, {\bf co-change relation}, and {\bf the depth limit on the stack trace and the execution path} that is the key parameter in our model. To evaluate each of them, we set the \tool as the full model, and each time we remove one key factor and compare the differences between the \tool and the \tool without one key factor $f$ to evaluate the impact of the key factor $f$. Because in this RQ, we only run the experiments based on \tool, we only do the set perdition setting. Except for the removed factor, we keep the same setting and parameters for all experiments to make a fair comparison.

{\bf RQ4. Evaluation on C Projects.}

To evaluate the performance of \tool on different programming languages. We also run the \tool on the C/C++ benchmark ManyBugs \cite{manybugs} with 185 bugs from 15 different projects. We run \tool on it with the same process as on the Java projects. Similar to RQ2 and RQ3, in this RQ, we only do the set prediction setting.
