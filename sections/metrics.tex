\subsubsection{Evaluation Metrics}

In this paper, \tool extracts a set of statements as the output while other state-of-the-art fault localization approaches often use the statement ranking lists with the ranking score for each statement as the output. To make a fair comparison. We have two separate settings to make the comparison:

{\bf Set Prediction Setting:} In this setting, we transfer the output from the state-of-the-art fault localization approaches to a set of statements as the output. To do so, for each bug $b_i$, we count the total number $k$ of statement in \tool's prediction results. And then, for the state-of-the-art fault localization approaches, we pick the top $k$ statements in the ranking list as a set to do the evaluation. In this setting, we have the following evaluation metrics:

\begin{itemize}
	\item {\bf $Hit n @Set$:} This metric means that in the predicted set, the percentage of bugs that the approach correctly predicted at least $n$ statements.
	\item {\bf $EXAM Score$\cite{wong2008crosstab}:} $EXAM score$ is the percentage of program statements a developer must manually checking before finding the first faulty statement. If there are ties, we think half of the ties are needed to do the manual checking.
	\item{\bf $EXAM\_AVG$:} Because the $EXAM score$ only considers the effort of finding the first faulty statement. In this paper, we create a new evaluation metrics $EXAM\_AVERAGE$. For a bug that contains $m$ statements that need to be fixed, we calculate the $EXAM score = {EXAM@1, EXAM@2, ..., EXAM@m}$ for finding the $1, 2, ..., m$ statements. And then we calculate the $EXAM\_AVERAGE = \frac{1}{m}\sum EXAM@m$
\end{itemize}

{\bf List Prediction Setting:} Similarly, we can also transfer our prediction results into the statement ranking list. To do so, we collect the classification scores for each statement together as a list instead of setting a threshold to pick out the statement set as the final output. In this case, the evaluation metrics are also a little bit different:

\begin{itemize}
	\item {\bf $Hit n @ Top K$:} This metric means that among the top $k$ in the statement ranking list, the number of bugs that the approach correctly predicted at least $n$ statements.
	\item {\bf $EXAM Score$\cite{wong2008crosstab} and $EXAM\_AVERAGE$:} These two metrics calculated in the same way as in the {\bf Set Prediction Setting}.
\end{itemize}
